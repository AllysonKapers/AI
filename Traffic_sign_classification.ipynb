{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "##pickle for serialization\n",
    "##glob searches for files that match a specific pattern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Resize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read and resize images, get labels and store them into np array\n",
    "def get_image_label_resize(label, filelist, dim=(32, 32), dataset='Train'):\n",
    "    x = np.array([cv2.resize(cv2.imread(fname), dim, interpolation=cv2.INTER_AREA) for fname in filelist])\n",
    "    y = np.array([label] * len(filelist))\n",
    "        \n",
    "    #print('{} examples loaded for label {}'.format(x.shape[0], label))\n",
    "    return (x, y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for label 0 \n",
    "# images that represent the first sign of 43 \n",
    "filelist = glob.glob(r\"C:\\Users\\ally_\\Downloads\\Traffic-Sign-Classifier-master\\Train\\0\\*.png\")\n",
    "trainx, trainy = get_image_label_resize(0, filelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize and Load Remaining Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through remaining folder labels and store images into np array\n",
    "for label in range(1,43):\n",
    "   \n",
    "    filelist = glob.glob(r\"C:\\Users\\ally_\\Downloads\\Traffic-Sign-Classifier-master\\Train\\\\\" + str(label) + r\"\\*.png\")\n",
    "    x, y = get_image_label_resize(label, filelist)\n",
    "    trainx = np.concatenate((trainx ,x))\n",
    "    trainy = np.concatenate((trainy ,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializes Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data into a pickle to later use\n",
    "# Serialization\n",
    "trainx.dump('../trainx.npy')\n",
    "trainy.dump('../trainy.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reads CSV, Resizes Images and Dumps Images and Corresponding ID's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path for test images\n",
    "testfile = pd.read_csv(r\"C:\\Users\\ally_\\Downloads\\Traffic-Sign-Classifier-master\\Test.csv\")\n",
    "file_names = list(testfile['Path'])\n",
    "\n",
    "#resizes and dumps test images\n",
    "X_test = np.array([cv2.resize(cv2.imread(fname), (32, 32), interpolation = cv2.INTER_AREA) for fname in file_names])\n",
    "X_test.dump('../testx.npy')\n",
    "\n",
    "#dumps images corresponding Class ID\n",
    "y_test = np.array(pd.read_csv('Test.csv')['ClassId'])\n",
    "y_test.dump('../testy.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deserializes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from pickle\n",
    "# Deserialization\n",
    "trainx = np.load('../Trainx.npy', allow_pickle=True)\n",
    "trainy = np.load('../Trainy.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from pickle\n",
    "X_test = np.load('../testx.npy', allow_pickle=True)\n",
    "y_test = np.load('../testy.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates Validation Set\n",
    "#### Training: Learning Model\n",
    "#### Validation: Prevents Overvitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training data and split them into training and validation\n",
    "indices = np.random.permutation(trainx.shape[0])\n",
    "# 20% to val\n",
    "split_idx = int(trainx.shape[0]*0.8)\n",
    "train_idx, val_idx = indices[:split_idx], indices[split_idx:]\n",
    "X_train, X_validation = trainx[train_idx,:], trainx[val_idx,:]\n",
    "y_train, y_validation = trainy[train_idx], trainy[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displays Data Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get overall stat of the whole dataset\n",
    "n_train = X_train.shape[0]\n",
    "n_validation = X_validation.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "image_shape = X_train[0].shape\n",
    "n_classes = len(np.unique(y_train))\n",
    "print(\"There are {} training examples \".format(n_train))\n",
    "print(\"There are {} validation examples\".format(n_validation))\n",
    "print(\"There are {} testing examples\".format(n_test))\n",
    "print(\"Image data shape is {}\".format(image_shape))\n",
    "print(\"There are {} classes\".format(n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converts to Grayscale and Normalizes\n",
    "#### Removes unnecessary data \"cleans it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the images to grayscale\n",
    "X_train_gry = np.sum(X_train/3, axis=3, keepdims=True)\n",
    "X_validation_gry = np.sum(X_validation/3, axis=3, keepdims=True)\n",
    "X_test_gry = np.sum(X_test/3, axis=3, keepdims=True)\n",
    "\n",
    "# Normalize data\n",
    "X_train_normalized_gry = (X_train_gry-128)/128\n",
    "X_validation_normalized_gry = (X_validation_gry-128)/128\n",
    "X_test_normalized_gry = (X_test_gry-128)/128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displays Difference in Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-colorblind')\n",
    "\n",
    "# descriptions for each label\n",
    "sign = pd.read_csv('signnames.csv')\n",
    "\n",
    "# pick an image, display the original and the normalized gray image\n",
    "index = np.random.randint(0, n_train)\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].set_title('original ' + sign.loc[sign['ClassId'] ==y_train[index], 'SignName'].values[0])\n",
    "ax[0].imshow(cv2.cvtColor(X_train[index], cv2.COLOR_BGR2RGB))\n",
    "\n",
    "ax[1].set_title('norm_gry ' + sign.loc[sign['ClassId'] ==y_train[index], 'SignName'].values[0])\n",
    "ax[1].imshow(X_train_normalized_gry[index].squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the train, val and test data with normalized gray images\n",
    "X_train = X_train_normalized_gry\n",
    "X_validation = X_validation_normalized_gry\n",
    "X_test = X_test_normalized_gry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start of Learning\n",
    "##### Activation Function(s): ReLU (rectified linear unit), Softmax (Output Layer)\n",
    "##### Conv2D: slides weights over data and provides matrix multiplication\n",
    "##### MaxPooling2D:https://keras.io/api/layers/pooling_layers/max_pooling2d/\n",
    "##### Flatten: flattens input, doesn't affect batch size\n",
    "##### Dense: each of the neurons of the dense layers receives input from all neurons of the previous layer\n",
    "##### Dropout:  for reducing over-fitting in neural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "# Conv 32x32x1 => 28x28x6.\n",
    "model.add(layers.Conv2D(filters = 6, kernel_size = (5, 5), strides=(1, 1), padding='valid', \n",
    "                        activation='relu', data_format = 'channels_last', input_shape = (32, 32, 1)))\n",
    "# Maxpool 28x28x6 => 14x14x6\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "# Conv 14x14x6 => 10x10x16\n",
    "model.add(layers.Conv2D(16, (5, 5), activation='relu'))\n",
    "# Maxpool 10x10x16 => 5x5x16\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "# Flatten 5x5x16 => 400\n",
    "model.add(layers.Flatten())\n",
    "# Fully connected 400 => 120\n",
    "model.add(layers.Dense(120, activation='relu'))\n",
    "# Fully connected 120 => 84\n",
    "model.add(layers.Dense(84, activation='relu'))\n",
    "# Dropout\n",
    "model.add(layers.Dropout(0.2))\n",
    "# Fully connected, output layer 84 => 43\n",
    "model.add(layers.Dense(43, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify optimizer, loss function and metric\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# training batch_size=128, epochs=10\n",
    "conv = model.fit(X_train, y_train, batch_size=128, epochs=10, \n",
    "                    validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = [conv.history['accuracy'], conv.history['val_accuracy']]\n",
    "loss = [conv.history['loss'], conv.history['val_loss']]\n",
    "\n",
    "epoch = range(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch, acc[0], label='Training Accuracy')\n",
    "plt.plot(epoch, acc[1], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch, loss[0], label='Training Loss')\n",
    "plt.plot(epoch, loss[1], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(0, n_test)\n",
    "im = X_test[index]\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(sign.loc[sign['ClassId'] ==np.argmax(model.predict(np.array([im]))), 'SignName'].values[0])\n",
    "ax.imshow(im.squeeze(), cmap = 'gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
